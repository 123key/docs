<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html lang="en-GB" xml:lang="en-GB" xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="content-type" content="text/html; charset=utf-8"/>
    <title>Introduction</title>
    <link rel="root" href="../../"/> <!-- for JS -->
    <link rel="stylesheet" type="text/css" href="../../css/jquery-ui-redmond.css"/>
    <link rel="stylesheet" type="text/css" href="../../css/style.css"/>
    <link rel="stylesheet" type="text/css" href="../../css/style-vis.css"/>
    <script type="text/javascript" src="../../lib/ext/head.load.min.js"></script>
<!--     <link rel="shortcut icon" href="favicon.ico"/> -->
  </head>
  <body>
    <div id="main" class="center">

      <div id="hp-header">
	
          <span class="header-text"><a href="http://universaldependencies.github.io/docs/#language-ja">home</a></span>

          <span class="header-text"><a href="https://github.com/universaldependencies/docs/edit/pages-source/_ja-overview/introduction.md" target="#">edit page</a></span>
          <span class="header-text"><a href="https://github.com/universaldependencies/docs/issues">issue tracker</a></span>
      </div>

      <hr/>

      <div id="content">
	<noscript>
	  <div id="noscript">
	    It appears that you have Javascript disabled.
	    Please consider enabling Javascript for this page to see the visualizations.
	  </div>
	</noscript>

          <h1 id="introduction">Introduction</h1>

<p>The Japanese corpora annotated according to the UD annotation scheme will be obtained by conversion from several resources.</p>

<p>As a first step, we construct conversion rules on ‘NTT Japanese phrase structure treebank’ (Tanaka and Nagata 2013) on Mainichi Newspaper.</p>

<p>We also try to construct conversion rules on ‘Balanced Corpus of Contemporary Written Japanese’(BCCWJ) (Maekawa et al. 2014) with third party annotations.</p>

<h1 id="basic-policy">Basic Policy</h1>

<p>Universal Dependency scheme does not suit for Japanese dependency annotation.
Their dependency annotation label set includes several different layers such as morphology, syntax dependency and semantic dependency.</p>

<p>Japanese language is written without word boundaries. We tend to define smaller morpheme unit than word unit in order to keep the unit uniformity. Therefore, when we define the morpheme unit as the Universal Dependency word unit, we have to annotate the compound word construction which is defined in the morphology layer of Japanese linguistics.</p>

<p>To split between the morphology issue and syntactic dependency issue, we define Japanese base phrase unit — Bunsetsu (文節) — for the syntactic dependency annotation.  The morphology issue includeing multi-word expression issue is encapsulated within the bunsetsu definition.  We can concentrate to annotate purely syntactic phenomena.  </p>

<p>The Part-of-speech in Japanese corpora can be split two philosophies: lexicon-based (語彙主義) and usage-based (用法主義).
Lexicon-based is to extract all possible categories for one word as a label.  For example, the label “名詞-普通名詞-サ変形状詞可能” means that the word can be Noun or Verbal Noun or Adjective. The label for the word is maintained in a large-scale PoS tagged lexicon and used in semi-markov model based morphological analyzers.
Usage-based is a label determined by the contextual information in the sentence.
We used Usage-based PoS tags from UniDic based lexicon/corpora/morphological analysers to align the Universal POS tags.</p>

<p>We also separate some issues — such as coordination structures, surface case frame, and scope of negation — from the Bunsetsu based dependency annotation.  </p>

<p>Coordination structures cannot be expressed straightforwardly in the dependency structures.
We lose some information related to nested coordination, non-constituent conjunct coordination and different syntactic category coordination by projecting the coordination structure to the dependency structures.
Therefore, we keep the coordinate structure information in the different layer annotation from the Bunsetsu based dependency annotation.
We also keep the surface case frame structures and the scope of negation in the different layer annotation.</p>

<p>The labels related to “PASSIVE” are defined in the Universal Dependency label set.  Hopefully, Japanese language has only two morphemes Reru(れる) and Rareru(られる). Though the morphemes are polysemy including the passive sense, 90% of them are passive sense.  We also have the annotation of the sense of Reru/Rareru.</p>

<p>The Universal Dependency label set discriminates whether the target is a clause or not.
Unfortunately, the definition of the clause is vague.
We defined some heuristic rules to define the clause.
For example, the difference between acl (adjectival clause) and amod (adjectival modifier) is defined by whether the adjective has any overt case or not.</p>

<p>The discrepancy between the syntactic phrases and phonetic (accent) phrases is another issues on the word-based dependency annotation.
We focus not on speech corpora but on written corpora.  We leave this issue from Universal Dependency annotation schema.</p>

<h1 id="background">Background</h1>

<p>Here, we describe Japanese language resources such as basic language resources, PoS tagged lexicon/corpus, morphological analyzers, syntactic dependency annotations, semantic dependency annotations (or case frame annotations), syntactic phrase structure tree annotations.</p>

<h2 id="corpora-with-annotations">Corpora with annotations</h2>

<ul>
  <li>
    <p>RWCP Corpus
‘RWCP Corpus’ is a Newspaper corpus from Mainichi Shinbun in 1994.
The corpus includes human-maintained Japanese word segmentation, morphological information and Part of Speech based on ‘IPADIC POS Tagset’.
The model of IPADIC and NAIST-jdic for ChaSen/MeCab are trained by RWCP Corpus.</p>
  </li>
  <li>
    <p>Kyoto Text Corpus
‘Kyoto Text Corpus’ is a Newspaper corpus from Mainichi Shinbun in 1995.
The corpus includes human-maintained Japanese word segmentation, morphological information and Part of Speech based on ‘Juman POS Tagset’.
The corpus also includes human-maintained syntactic dependency annotation (‘Kyoto Text Corpus Standard’) and case frame annotation. The ‘NTT Japanese phrase structure treebank’ is a phrase structure treebank based on the Kyoto Text Corpus.
‘NAIST Text Corpus’ is a case frame and coreference annotations on the Kyoto Text Corpus.</p>
  </li>
</ul>

<p>Kyoto Text Corpus: <a href="http://nlp.ist.i.kyoto-u.ac.jp/index.php?%E4%BA%AC%E9%83%BD%E5%A4%A7%E5%AD%A6%E3%83%86%E3%82%AD%E3%82%B9%E3%83%88%E3%82%B3%E3%83%BC%E3%83%91%E3%82%B9">page</a></p>

<p>NAIST Text Corpus: <a href="https://sites.google.com/site/naisttextcorpus/">https://sites.google.com/site/naisttextcorpus/</a></p>

<ul>
  <li>
    <p>EDR Corpus
‘EDR Corpus’ includes Japanese and English corpora.
The Japanese part of the corpora includes human-maintained  morphological information, Part of Speech based on ‘EDR POS Tagset’, syntactic phrase structure, and semantic frame information.</p>
  </li>
  <li>
    <p>CSJ
‘Corpus of Spontaneous Japanese’ (CSJ) is a monologue speech corpus.
The corpus includes the original speech sound files, transcripted text, clause boundaries, morphological information, and Part of Speech based on ‘UniDic POS Tagset’.
CSJ defines several layers of word segmentation such as ‘Short Unit Word’ which is the morphological unit for uniformity, ‘Middle Unit Word’ which is compound word left branching tree to form an accent phrase, and ‘Long Unit Word’ which composes a maximal content word and some functional words with in a syntactic base phrase — Bunsetsu.
The corpus also includes Bunsetsu-based dependency (‘CSJ standard’).</p>
  </li>
</ul>

<p>Corpus of Spontaneous Japanese: <a href="http://pj.ninjal.ac.jp/corpus_center/csj/">http://pj.ninjal.ac.jp/corpus_center/csj/</a></p>

<ul>
  <li>BCCWJ
‘Balanced Corpus of Contemporary Written Japanese’ (BCCWJ) is a 100 million scale corpus which consists of three subcorpora. The first one is called the Publication Subcorpus. Samples of this corpus are extracted randomly from the population of all books, magazines, and major newspapers published in the years 2001-2005.
The second one is called the Library Subcorpus. Its population consists of all books that are catalogued at more than 13 metropolitan libraries in Tokyo.
The third one is called the Special-purpose Subcorpus. This corpus contains a series of mutually unrelated mini corpora that are required for specific research purposes of the NINJAL research groups. The mini corpora include governmental white papers, textbooks, laws, bestselling books, and text from the Internet (provided by the courtesy of Yahoo! Japan Inc). Each of these mini corpora contains text of several million words.
A part of BCCWJ is called “CORE” data which contains one million words from newspaper, books, magazines, whitepaper, Yahoo! Answers, and Yahoo! Blog.
The CORE part includes hand-maintained sentence boundaries, word boundaries, morphological information and Part of Speech based on ‘UniDic POS Tagset’.  Some researchers provide annotations on BCCWJ CORE data in several layers such as Bunsetsu-based dependency (‘BCCWJ-DepPara standard’), coordination structure, usage of passive auxiliary verb, scope of negation, aspects, TimeML TIMEX3, TimeML TLINK, predicate argument structure, case frame structure, Japanese framenet, extended named entities, and word senses.</li>
</ul>

<p>Balanced Corpus of Contemporary Written Japanese: <a href="http://pj.ninjal.ac.jp/corpus_center/bccwj/">http://pj.ninjal.ac.jp/corpus_center/bccwj/</a></p>

<ul>
  <li>KNBC Corpus (free)
‘KNBC Corpus’ is a copyright free Blog corpus.  The corpus includes JUMAN POS tags and Bunsetsu-based dependency in Kyoto Text corpus standard.</li>
</ul>

<h2 id="word-unit">Word Unit</h2>

<h3 id="overview-of-word-unit">Overview of Word Unit</h3>
<p>Japanese sentences are not split into words or morphemes by spaces.
We have several word unit standards by corpus annotation schema or morphological analysers’ outputs.
The representative word unit standards are the following subsubsections.</p>

<h3 id="ipadic-word-unit">IPADIC word unit:</h3>
<p>This word unit standard was derived by the morphological analyzer ChaSen. IPADIC is one of morphological information annotated lexicon used in ChaSen.  The morphological analyzer MeCab is developed in 2001-2004. MeCab is independently developped from the lexicon. However, the default lexicon is IPADIC. NAIST-jdic is a successor of IPADIC. NAIST-jdic resolves the license issues in IPADIC. NAIST-jdic inherits the word unit definitiosn and POS tagset of IPADIC. </p>

<p>IPADIC legacy: <a href="http://sourceforge.jp/projects/ipadic/">http://sourceforge.jp/projects/ipadic/</a></p>

<p>NAIST-jdic: <a href="http://sourceforge.jp/projects/naist-jdic/">http://sourceforge.jp/projects/naist-jdic/</a></p>

<h3 id="ninjal-unidic-word-units">NINJAL UniDic word units:</h3>
<p>NINJAL (National Institute for Japanese Language and Linguistics, Japan) proposed several word unit standards for Japanese corpus linguistics such as minimum word unit, \alpha word unit, \beta word unit, M word unit and so on.  From 2002, they maintain a morporlogical information annotated lexicon UniDic, and propose three sorts of word unit standard:
* Short Unit Word (SUW): 
The standard is used as the most fine grained morpheme for the corpus statistics. The word unit is derived by MeCab with UniDic.
* Middle Unit Word (MUW): The standard is based on the left branching compound word construction. The word unit is based on phonological construction such as accent phrase and/or sequential voicing(連濁). The unit is derived by Comainu which is a wrapper program of MST parsing.
* Long Unit Word (LUW): The standard is composed “Bunsetsu(文節)” unit. LUW is nearly same as content or functional words bounded by bunsetsu boundaries and The unit is derives by SVM-based chunker such as YamCha or linear chain CRF chunker such as CRF++.
The UniDic are maintained diachronically. NINJAL published versions of UniDic in several eras.</p>

<ul>
  <li>UniDic (contemporary): <a href="http://sourceforge.jp/projects/unidic/">http://sourceforge.jp/projects/unidic/</a> will be moved to <a href="http://download.unidic.org/">http://download.unidic.org/</a></li>
  <li>UniDic (modern): <a href="http://www2.ninjal.ac.jp/lrc/index.php?UniDic%2F%B6%E1%C2%E5%CA%B8%B8%ECUniDic">http://www2.ninjal.ac.jp/lrc/index.php?UniDic%2F%B6%E1%C2%E5%CA%B8%B8%ECUniDic</a></li>
  <li>UniDic (early middle): <a href="http://www2.ninjal.ac.jp/lrc/index.php?UniDic%2F%C3%E6%B8%C5%CF%C2%CA%B8UniDic">http://www2.ninjal.ac.jp/lrc/index.php?UniDic%2F%C3%E6%B8%C5%CF%C2%CA%B8UniDic</a></li>
</ul>

<h3 id="jumandic-word-unit">JUMANdic word unit:</h3>
<p>This word unit standard was derived by the morphological analyzer JUMAN. The unit is longer than SUW in UniDic.</p>

<h3 id="morphological-analysers">Morphological Analysers</h3>

<ul>
  <li>MeCab: lexicon-based morphological analyzer by CRFs.</li>
</ul>

<p>MeCab: <a href="http://mecab.googlecode.com/svn/trunk/mecab/doc/index.html">http://mecab.googlecode.com/svn/trunk/mecab/doc/index.html</a></p>

<ul>
  <li>JUMAN: lexicon-based morphological analyzer by hand maintained cost table.</li>
</ul>

<p>JUMAN: <a href="http://nlp.ist.i.kyoto-u.ac.jp/index.php?JUMAN">http://nlp.ist.i.kyoto-u.ac.jp/index.php?JUMAN</a></p>

<ul>
  <li>Comainu: compound word construction by chunking (CRF or history-based SVM) and dependency parsing model.</li>
</ul>

<p>Comainu: <a href="http://comainu.org/">http://comainu.org/</a></p>

<ul>
  <li>CRF chunker 
CRF++: <a href="http://taku910.github.io/crfpp/">http://taku910.github.io/crfpp/</a></li>
  <li>History based chunker
YamCha: <a href="http://chasen.org/~taku/software/yamcha/">http://chasen.org/~taku/software/yamcha/</a></li>
</ul>

<h2 id="bunsetsu-unit-base-phrase">Bunsetsu Unit (Base phrase)</h2>
<p>We use NINJAL Short Unit Word (SUW) in UniDic. (ref. UniDic)</p>

<h2 id="pos-tag">PoS Tag</h2>

<p>UD PoS for Japanese is obtained by conversion rules from UniDic PoS Tagset.</p>

<h3 id="ipadic-pos-tagset">IPADIC POS Tagset</h3>
<p>IPADIC and NAIST-jdic are shared the same POS tagset.
- The English Translation of POS Tagset by Dr. Francis Bond: http://sourceforge.jp/projects/ipadic/docs/postag.txt/ja/1/postag.txt.txt
- English Manual: http://sourceforge.jp/projects/ipadic/docs/ipadic-2.7.0-manual-en.pdf/en/1/</p>

<h3 id="juman-pos-tagset">Juman POS Tagset</h3>

<h3 id="unidic-pos-tagset">UniDic POS Tagset</h3>

<h3 id="lexicon-based-or-usage-based">Lexicon-based or Usage-based</h3>

<p>The term ‘usage-based’ is not exactly same as Langacker’s Usage-based model.</p>

<h3 id="issues-on-ud-pos-tagset">Issues on UD PoS tagset</h3>

<p>The UD doesn’t define lexicon-based or usage-based PoS tagset.</p>

<h2 id="dependency-links">Dependency Links</h2>

<h3 id="general-description">General Description</h3>
<ul>
  <li>Strictly Head Final</li>
  <li>Arrow from modifier to head 
### Annotation Schema</li>
  <li>Kyoto Corpus Schema
Label: D, P, I, A</li>
  <li>CSJ Schema</li>
  <li>BCCWJ-DepPara Schema</li>
  <li>Word Dependency by Mori (Mori et al. 2014)</li>
</ul>

<h3 id="bunsetsu-based-syntactic-dependency-parser">Bunsetsu-based Syntactic Dependency Parser</h3>
<ul>
  <li>KNP: <a href="http://nlp.ist.i.kyoto-u.ac.jp/index.php?KNP">http://nlp.ist.i.kyoto-u.ac.jp/index.php?KNP</a></li>
  <li>CaboCha: <a href="http://taku910.github.io/cabocha/">http://taku910.github.io/cabocha/</a></li>
  <li>jdepp: <a href="http://www.tkl.iis.u-tokyo.ac.jp/~ynaga/jdepp/">http://www.tkl.iis.u-tokyo.ac.jp/~ynaga/jdepp/</a></li>
  <li>Yahoo! API: <a href="http://developer.yahoo.co.jp/webapi/jlp/da/v1/parse.html">http://developer.yahoo.co.jp/webapi/jlp/da/v1/parse.html</a>
## Other Dependency Relations</li>
</ul>

<h2 id="semantic-dependency-annotation-schema-with-corpus-semantic-dependency-parser">Semantic Dependency Annotation Schema (with corpus), Semantic Dependency Parser</h2>
<ul>
  <li>Kyoto Corpus Schema (on Mainichi Newspaper)</li>
  <li>NAIST Corpus Schema (on Mainichi Newspaper and BCCWJ)</li>
</ul>

<h2 id="other-annotations-syntax-related-only">Other annotations (syntax related only)</h2>
<ul>
  <li>Scope of Negation for BCCWJ</li>
  <li>Passive Auxiliary Verb for BCCWJ</li>
</ul>

<h2 id="issues-general">Issues General</h2>
<ul>
  <li>Both of Mainichi Newspaper and BCCWJ are non-free resources.</li>
</ul>

<h1 id="contributors">Contributors</h1>

<ul>
  <li>Yusuke Miyao</li>
  <li>Hiroshi Kanayama</li>
  <li>Takaaki Tanaka</li>
  <li>Shinsuke Mori</li>
  <li>Sumire Uematsu</li>
  <li>Masayuki Asahara</li>
</ul>

<h1 id="references">References</h1>

<h2 id="lexicon">Lexicon</h2>
<ul>
  <li>Yasuharu Den, Junichi Nakamura, Toshinobu Ogiso and Hideki Ogura. 2008. ‘<a href="http://www.lrec-conf.org/proceedings/lrec2008/pdf/258_paper.pdf">A proper approache to Japanese morphological analysis: Dictionary, model and evaluation.</a>’. In Proceedings of the 6th Language Resources and Evaluation Conference (LREC-2008).</li>
  <li>Toshinobu Ogiso, Mamoru Komachi, Yasuharu Den and Yuji Matsumoto, 2012 ‘<a href="http://www.lrec-conf.org/proceedings/lrec2012/pdf/906_Paper.pdf">UniDic for Early Middle Japanese: a Dictionary for Morphological Analysis of Classical Japanese</a>’. In Proceedings of the Eighth International Conference on Language Resources and Evaluation Conference (LREC-2012).</li>
</ul>

<h2 id="corpora">Corpora</h2>
<ul>
  <li>Takaaki Tanaka and Masaaki Nagata. 2013. ‘<a href="http://www.aclweb.org/anthology/W13-4913">Constructing a Practical Constituent Parser from a Japanese Treebank with Function Labels</a>’ In Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically-Rich Languages</li>
  <li>Kikuo Maekawa, Makoto Yamazaki, Toshinobu Ogiso, Takehiko Maruyama, Hideki Ogura, Wakako Kashino, Hanae Koiso, Masaya Yamaguchi, Makiro Tanaka, and Yasuharu Den. 2014. ‘<a href="http://link.springer.com/article/10.1007/s10579-013-9261-0">Balanced corpus of contemporary written Japanese</a>’. Language Resources and Evaluation 48 (2), pp.345-371</li>
</ul>



      </div>

<!-- support for embedded visualizations -->
<script type="text/javascript">
    var root = '../../'; // filled in by jekyll
    head.js(
        // External libraries
        root + 'lib/ext/jquery.min.js',
        root + 'lib/ext/jquery.svg.min.js',
        root + 'lib/ext/jquery.svgdom.min.js',
        root + 'lib/ext/jquery-ui.min.js',
        root + 'lib/ext/waypoints.min.js',
        root + 'lib/ext/jquery.address.min.js',

        // brat helper modules
        root + 'lib/brat/configuration.js',
        root + 'lib/brat/util.js',
        root + 'lib/brat/annotation_log.js',
        root + 'lib/ext/webfont.js',
        // brat modules
        root + 'lib/brat/dispatcher.js',
        root + 'lib/brat/url_monitor.js',
        root + 'lib/brat/visualizer.js',

        // embedding configuration
        root + 'lib/local/config.js',
        // project-specific collection data
        root + 'lib/local/collections.js',

        // NOTE: non-local libraries
        'http://spyysalo.github.io/annodoc/lib/local/annodoc.js',
        'http://spyysalo.github.io/conllu.js/conllu.js'
    );

    var webFontURLs = [
//        root + 'static/fonts/Astloch-Bold.ttf',
        root + 'static/fonts/PT_Sans-Caption-Web-Regular.ttf',
        root + 'static/fonts/Liberation_Sans-Regular.ttf'
    ];

    var setupTabs = function() {
        // standard jQuery UI "tabs" element initialization
        $(".jquery-ui-tabs").tabs({ heightStyle: "auto" });

        // use jQuery address to preserve tab state
        // (see https://github.com/UniversalDependencies/docs/issues/65,
        // http://stackoverflow.com/a/3330919)
        if ($(".jquery-ui-tabs").length > 0) {
            $.address.change(function(event){
	        $(".jquery-ui-tabs").tabs("select", window.location.hash)
	    });
	    $(".jquery-ui-tabs").bind("tabsselect", function(event, ui) { 
	        window.location.hash = ui.tab.hash;
	    });
        }
    };

    head.ready(function() {
        // set up UI tabs on page
        setupTabs();

        // mark current collection (filled in by Jekyll)
        Collections.listing['_current'] = 'ja-overview';

	// perform all embedding and support functions
	Annodoc.activate(Config.bratCollData, Collections.listing);
    });
</script>


<!-- google analytics -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-55233688-1', 'auto');
  ga('send', 'pageview');

</script>


      <div id="footer">
	  <p class="footer-text">&copy; 2014 
	    <a href="http://universaldependencies.github.io/docs/introduction.html#contributors" style="color:gray">Universal Dependencies contributors</a>. 
	    Site powered by <a href="http://spyysalo.github.io/annodoc" style="color:gray">Annodoc</a> and <a href="http://brat.nlplab.org/" style="color:gray">brat</a></p>.
      </div>
    </div>
  </body>
</html>
